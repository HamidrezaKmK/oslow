{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from notebook_setup import device, smooth_graph, create_new_set_of_models, train_models_and_get_histories, update_dict\n",
    "from oslow.models.oslow import OSlow\n",
    "from oslow.data.synthetic.graph_generator import GraphGenerator\n",
    "from oslow.data.synthetic.utils import RandomGenerator\n",
    "from oslow.data.synthetic.parametric import AffineParametericDataset\n",
    "from oslow.models.normalization import ActNorm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to GPU k\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a causal graph using the GraphGenerator class. Here, we specify the number of nodes (3) and enforce a specific ordering [1, 0, 2]. This graph will be used as the ground truth for our causal discovery experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_covariates = 4\n",
    "true_ordering = [1, 2, 0, 3] # [2, 1, 0] # [1, 3, 0, 2, 4]\n",
    "graph_generator = GraphGenerator(\n",
    "    num_nodes=num_covariates,\n",
    "    seed=0,\n",
    "    graph_type=\"full\",\n",
    "    enforce_ordering=true_ordering,\n",
    ")\n",
    "graph = graph_generator.generate_dag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we generate synthetic data based on the causal graph. We create an AffineParametericDataset \n",
    "with sinusoidal links between variables. This dataset will be used to train our OSlow models and \n",
    "test our causal discovery strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 50000\n",
    "gaussian_noise_generator = RandomGenerator('normal', seed=10, loc=0, scale=1)\n",
    "link_generator = RandomGenerator('uniform', seed=110, low=1, high=1)\n",
    "\n",
    "dset_sinusoidal = AffineParametericDataset(\n",
    "    num_samples=num_samples,\n",
    "    graph_generator=graph_generator,  # Not graph, requires a GraphGenerator object that generates the DAG\n",
    "    noise_generator=gaussian_noise_generator,\n",
    "    link_generator=link_generator,\n",
    "    link=\"sinusoid\",\n",
    "    perform_normalization=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the settings for our OSlow models and their training process. We specify the model \n",
    "architecture (additive or not, number of transforms, normalization method) and training parameters \n",
    "(batch size, learning rate, number of epochs). These settings will be used for all OSlow models we create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_instantiation_setting = dict(\n",
    "    additive = False,\n",
    "    num_transforms = 1,\n",
    "    normalization = ActNorm,\n",
    "    base_distribution = torch.distributions.Normal(loc=0, scale=1),\n",
    "    use_standard_ordering=False,\n",
    ")\n",
    "\n",
    "base_training_setting = dict(\n",
    "    batch_size=512,\n",
    "    lr=0.005,\n",
    "    epoch_count=10,\n",
    "    use_standard_ordering=False,\n",
    ")\n",
    "batch_size = base_training_setting['batch_size']\n",
    "lr = base_training_setting['lr']\n",
    "epoch_count = base_training_setting['epoch_count']\n",
    "use_standard_ordering = base_training_setting['use_standard_ordering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_samples = torch.tensor(dset_sinusoidal.samples.values).float()\n",
    "torch_dataset = TensorDataset(tensor_samples)\n",
    "torch_dataloader = DataLoader(torch_dataset, batch_size=base_training_setting['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Check\n",
    "\n",
    "Check that the a model conditioned on the true ordering of covariates actually corresponds to the highest log-likelihood (lowest loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # comment to select the ones you want to plot\n",
    "# subset_to_consider = [\n",
    "#     \"sinusoidal\", # only show the last 0.1 of the epochs\n",
    "# ]\n",
    "# all_models_trained = {}\n",
    "# for key in subset_to_consider:\n",
    "#     print(\"key: \", key)\n",
    "#     print(\"dataset: \", dset_sinusoidal)\n",
    "#     print(\"all_models_trained: \", all_models_trained)\n",
    "#     # print(\"all_models_trained[key]: \", all_models_trained[key])\n",
    "#     dset = dset_sinusoidal\n",
    "#     all_models_trained[key] = create_new_set_of_models(**base_model_instantiation_setting)\n",
    "#     all_histories = train_models_and_get_histories(**base_training_setting, dset=dset, all_models=all_models_trained[key])\n",
    "\n",
    "#     smoothed_histories = smooth_graph(all_histories, window_size=100)\n",
    "#     # create two subplots and unpack the output array immediately\n",
    "#     plt.figure(figsize=(15, 5))\n",
    "#     plt.subplot(121)\n",
    "#     plt.title(f\"full loss graph {key}\")\n",
    "#     plt.xlabel(\"epochs\")\n",
    "#     plt.ylabel(\"loss\")\n",
    "#     for order in all_histories.keys():\n",
    "#         plt.plot(all_histories[order], label=order)\n",
    "#     plt.legend()\n",
    "#     plt.subplot(122)\n",
    "#     plt.xlabel(\"epochs\")\n",
    "#     plt.ylabel(\"loss\")\n",
    "#     for order in all_histories.keys():\n",
    "#         ending_portion = int(0.1 * len(smoothed_histories[order]))\n",
    "#         plt.plot(smoothed_histories[order][-ending_portion:], label=order)\n",
    "#     plt.title(f\"{key} ending portion smoothed\")\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_oslow_model_with_ordering(ordering):\n",
    "    return OSlow(\n",
    "        in_features=len(ordering),\n",
    "        layers=[100, 100],\n",
    "        dropout=None,\n",
    "        residual=False,\n",
    "        activation=torch.nn.LeakyReLU(),\n",
    "        additive=False,\n",
    "        num_transforms=1,\n",
    "        normalization=ActNorm,\n",
    "        base_distribution=torch.distributions.Normal(loc=0, scale=1),\n",
    "        ordering=torch.tensor(ordering)  # Pass the ordering here\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Ordering algorithm\n",
    "\n",
    "See the determined_ordering function. This is a recursive algorithm to determine the causal ordering of covariates.\n",
    "\n",
    "* For each starting covariate, we will sample the remaining covariates to form complete permutations of length num_total_covariates by sampling uniformly without replacement. We will create and train a unique OSLow model for each permutation that we have sampled.\n",
    "* Once training has finished, for each starting covariate, calculate the average of the log probabilities of the data over all the OSLow models with that starting covariate.\n",
    "* The starting covariate used in the OSLow models achieving the lowest log probability on average (in expectation) indicates which covariate comes first.\n",
    "* Fix the first element and then continue this recursively until all the elements have been ordered.\n",
    "\n",
    "For example, if we have 3 variables [0, 1, 2], then for the starting covariate 0, we would have the models conditioned on permutations [0, 1, 2] and [0, 2, 1]. We would take the final log probability for the [0, 1, 2] model and the final log probability for the [0, 2, 1] model and find their average final log probability by taking their mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_permutations(remaining):\n",
    "    \"\"\"\n",
    "    From a list or tuple of remaining covariates, return a list of all possible permutations, each of which is a list.\n",
    "\n",
    "    Returns:\n",
    "    List[List[int]]: List of all possible permutations of the remaining covariates\n",
    "    \"\"\"\n",
    "    return [list(p) for p in permutations(remaining)]\n",
    "\n",
    "def initialize_models_and_optimizers(determined_ordering, remaining_covariates, num_total_covariates):\n",
    "    \"\"\"\n",
    "    Initialize models, optimizers, and histories for all possible permutations of remaining covariates.\n",
    "\n",
    "    Args:\n",
    "    - determined_ordering: list of covariates that have already been ordered\n",
    "    - remaining_covariates: list of remaining covariates to consider\n",
    "    - num_total_covariates: total number of covariates in the dataset\n",
    "\n",
    "    Returns:\n",
    "    - models: dictionary of OSlow models for each permutation\n",
    "    - optimizers: dictionary of optimizers for each model\n",
    "    - histories: dictionary to store training histories for each model\n",
    "    \"\"\"\n",
    "    all_permutations = sample_permutations(remaining_covariates)\n",
    "    models = {}\n",
    "    histories = {}\n",
    "\n",
    "    for perm in all_permutations:\n",
    "        full_perm = determined_ordering + perm\n",
    "        perm_key = tuple(full_perm)  # Use tuple as dictionary key\n",
    "\n",
    "        # Create model\n",
    "        model = create_oslow_model_with_ordering(full_perm).to(device)\n",
    "        models[perm_key] = model\n",
    "\n",
    "        # Initialize history\n",
    "        histories[perm_key] = []\n",
    "\n",
    "        # Create initial permutation matrix\n",
    "        perm_matrix = torch.zeros((num_total_covariates, num_total_covariates))\n",
    "        for i, j in enumerate(full_perm):\n",
    "            perm_matrix[i, j] = 1\n",
    "        perm_matrix = perm_matrix.to(device)\n",
    "\n",
    "        # Store permutation matrix\n",
    "        models[perm_key].perm_matrix = perm_matrix\n",
    "\n",
    "    return models, histories\n",
    "\n",
    "\n",
    "def determine_ordering(remaining_covariates, determined_ordering=None, num_total_covariates=None, true_ordering=None, true_model=None, depth=0):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - remaining_covariates: list of remaining covariates to consider\n",
    "    - determined_ordering: list of covariates that have already been ordered\n",
    "    - num_total_covariates: total number of covariates in the dataset (should be 3)\n",
    "    - true_ordering: list of true ordering of covariates\n",
    "    - true_model: a model that is conditioned on the true ordering\n",
    "    - depth: current depth of recursion\n",
    "\n",
    "    Determine the correct causal ordering greedily by choosing the best covariate to come next.\n",
    "    Do this by searching over the remaining covariates and for each covariate, run the following steps:\n",
    "    For each covariate, sample the remaining covariates to form complete permutations of length num_total_covariates by sampling uniformly without replacement. \n",
    "    We will create and train a unique OSLow model for each permutation that we have sampled.\n",
    "    Once training has finished, for each starting covariate, calculate the average of the log probabilities of the data over all the OSLow models with that starting covariate.\n",
    "    \n",
    "    The starting covariate used in the OSLow models achieving the lowest log probability on average (in expectation) indicates which covariate comes first.\n",
    "    Fix the first element and then continue this recursively until all the elements have been ordered.\n",
    "\n",
    "    Output:\n",
    "    - determined_ordering: list of covariates in the determined ordering\n",
    "    \"\"\"\n",
    "    if determined_ordering is None:\n",
    "        determined_ordering = []\n",
    "    if num_total_covariates is None:\n",
    "        num_total_covariates = len(remaining_covariates)\n",
    "\n",
    "    if len(remaining_covariates) == 1:\n",
    "        return determined_ordering + remaining_covariates\n",
    "\n",
    "    print(f\"\\nCurrent stage of recursion:\")\n",
    "    print(f\"  Ground truth ordering: {true_ordering}\")\n",
    "    print(f\"  Fixed ordering so far: {determined_ordering}\")\n",
    "    print(f\"  Remaining covariates: {remaining_covariates}\")\n",
    "\n",
    "    # Initialize models and optimizers for the remaining covariates\n",
    "    models, histories = initialize_models_and_optimizers(determined_ordering, remaining_covariates, num_total_covariates)\n",
    "    \n",
    "    # Create true model only once at the start of recursion\n",
    "    if true_ordering is not None and true_model is None:\n",
    "        true_model = create_oslow_model_with_ordering(true_ordering).to(device)\n",
    "        true_optimizer = torch.optim.Adam(true_model.parameters(), lr=lr)\n",
    "\n",
    "    i = 0\n",
    "    print(\"models.keys(): \", models.keys())\n",
    "    for perm_key in tqdm(models, desc=f\"Training models for {len(remaining_covariates)} remaining covariates\"):\n",
    "        model = models[perm_key]\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(epoch_count):\n",
    "            for batch, in torch_dataloader:\n",
    "                batch = batch.to(device)\n",
    "                log_prob = model.log_prob(batch).mean()\n",
    "                loss = -log_prob\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                histories[perm_key].append(log_prob.item())\n",
    "            \n",
    "                # Train true ordering model only once at the start\n",
    "                if true_ordering is not None and true_model is not None and determined_ordering == [] and i == 0:\n",
    "                    true_log_prob = true_model.log_prob(batch).mean()\n",
    "                    true_loss = -true_log_prob\n",
    "                    \n",
    "                    true_optimizer.zero_grad()\n",
    "                    true_loss.backward()\n",
    "                    true_optimizer.step()\n",
    "        i += 1\n",
    "        \n",
    "    if determined_ordering == [] and true_model is not None:\n",
    "        print(\"Final log probability of true model after training: \", true_log_prob.item())\n",
    "\n",
    "    # final_log_probs = {perm_key: histories[perm_key][-1] for perm_key in models}\n",
    "    # best_perm_key = max(final_log_probs, key=final_log_probs.get)\n",
    "    # next_covariate = best_perm_key[len(determined_ordering)]\n",
    "\n",
    "    # Calculate average log probabilities for each starting covariate\n",
    "    starting_covariate_avg_log_probs = {}\n",
    "    for covariate in remaining_covariates:\n",
    "        relevant_perms = [perm for perm in models.keys() if perm[len(determined_ordering)] == covariate]\n",
    "        print(f\"  Covariate {covariate} has {len(relevant_perms)} relevant permutations\")\n",
    "        print(f\"  Relevant permutations: {relevant_perms}\")\n",
    "        avg_log_prob = sum(histories[perm][-1] for perm in relevant_perms) / len(relevant_perms)\n",
    "        starting_covariate_avg_log_probs[covariate] = avg_log_prob\n",
    "\n",
    "    # Choose the best starting covariate\n",
    "    next_covariate = max(starting_covariate_avg_log_probs, key=starting_covariate_avg_log_probs.get)\n",
    "    \n",
    "    # Sanity check\n",
    "    if true_ordering is not None and true_model is not None:\n",
    "        print(f\"\\nSanity Check Results at depth {depth}:\")\n",
    "        print(f\"  Best found average log probability: {starting_covariate_avg_log_probs[next_covariate]}\")\n",
    "        print(f\"  Average log probabilities for each starting covariate:\")\n",
    "        for covariate, avg_log_prob in starting_covariate_avg_log_probs.items():\n",
    "            print(f\"    Covariate {covariate}: {avg_log_prob}\")\n",
    "        # print(f\"  Best found permutation: {best_perm_key}\")\n",
    "        print(f\"  Best covariate at this stage: {next_covariate}\")\n",
    "        print(f\"  Current ordering so far (including new covariate): {determined_ordering + [next_covariate]}\")\n",
    "        print(f\"  True ordering so far (including new covariate): {true_ordering[:depth+1]}\")\n",
    "    \n",
    "    remaining_covariates = [i for i in remaining_covariates if i != next_covariate]\n",
    "    return determine_ordering(remaining_covariates, determined_ordering + [next_covariate], num_total_covariates, true_ordering, true_model, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current stage of recursion:\n",
      "  Fixed ordering so far: []\n",
      "  Remaining covariates: [0, 1, 2, 3]\n",
      "models.keys():  dict_keys([(0, 1, 2, 3), (0, 1, 3, 2), (0, 2, 1, 3), (0, 2, 3, 1), (0, 3, 1, 2), (0, 3, 2, 1), (1, 0, 2, 3), (1, 0, 3, 2), (1, 2, 0, 3), (1, 2, 3, 0), (1, 3, 0, 2), (1, 3, 2, 0), (2, 0, 1, 3), (2, 0, 3, 1), (2, 1, 0, 3), (2, 1, 3, 0), (2, 3, 0, 1), (2, 3, 1, 0), (3, 0, 1, 2), (3, 0, 2, 1), (3, 1, 0, 2), (3, 1, 2, 0), (3, 2, 0, 1), (3, 2, 1, 0)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models for 4 remaining covariates: 100%|██████████| 24/24 [06:01<00:00, 15.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final log probability of true model after training:  -10.455864906311035\n",
      "  Covariate 0 has 6 relevant permutations\n",
      "  Relevant permutations: [(0, 1, 2, 3), (0, 1, 3, 2), (0, 2, 1, 3), (0, 2, 3, 1), (0, 3, 1, 2), (0, 3, 2, 1)]\n",
      "  Covariate 1 has 6 relevant permutations\n",
      "  Relevant permutations: [(1, 0, 2, 3), (1, 0, 3, 2), (1, 2, 0, 3), (1, 2, 3, 0), (1, 3, 0, 2), (1, 3, 2, 0)]\n",
      "  Covariate 2 has 6 relevant permutations\n",
      "  Relevant permutations: [(2, 0, 1, 3), (2, 0, 3, 1), (2, 1, 0, 3), (2, 1, 3, 0), (2, 3, 0, 1), (2, 3, 1, 0)]\n",
      "  Covariate 3 has 6 relevant permutations\n",
      "  Relevant permutations: [(3, 0, 1, 2), (3, 0, 2, 1), (3, 1, 0, 2), (3, 1, 2, 0), (3, 2, 0, 1), (3, 2, 1, 0)]\n",
      "\n",
      "Sanity Check Results at depth 0:\n",
      "  Best found average log probability: -10.748207569122314\n",
      "  Average log probabilities for each starting covariate:\n",
      "    Covariate 0: -10.938713391621908\n",
      "    Covariate 1: -10.748207569122314\n",
      "    Covariate 2: -10.842563470204672\n",
      "    Covariate 3: -11.98186206817627\n",
      "  Best covariate at this stage: 1\n",
      "  Current ordering so far (including new covariate): [1]\n",
      "  True ordering so far (including new covariate): [1]\n",
      "\n",
      "Current stage of recursion:\n",
      "  Fixed ordering so far: [1]\n",
      "  Remaining covariates: [0, 2, 3]\n",
      "models.keys():  dict_keys([(1, 0, 2, 3), (1, 0, 3, 2), (1, 2, 0, 3), (1, 2, 3, 0), (1, 3, 0, 2), (1, 3, 2, 0)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models for 3 remaining covariates: 100%|██████████| 6/6 [01:14<00:00, 12.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Covariate 0 has 2 relevant permutations\n",
      "  Relevant permutations: [(1, 0, 2, 3), (1, 0, 3, 2)]\n",
      "  Covariate 2 has 2 relevant permutations\n",
      "  Relevant permutations: [(1, 2, 0, 3), (1, 2, 3, 0)]\n",
      "  Covariate 3 has 2 relevant permutations\n",
      "  Relevant permutations: [(1, 3, 0, 2), (1, 3, 2, 0)]\n",
      "\n",
      "Sanity Check Results at depth 1:\n",
      "  Best found average log probability: -10.623709678649902\n",
      "  Average log probabilities for each starting covariate:\n",
      "    Covariate 0: -10.669552326202393\n",
      "    Covariate 2: -10.623709678649902\n",
      "    Covariate 3: -10.976052284240723\n",
      "  Best covariate at this stage: 2\n",
      "  Current ordering so far (including new covariate): [1, 2]\n",
      "  True ordering so far (including new covariate): [1, 2]\n",
      "\n",
      "Current stage of recursion:\n",
      "  Fixed ordering so far: [1, 2]\n",
      "  Remaining covariates: [0, 3]\n",
      "models.keys():  dict_keys([(1, 2, 0, 3), (1, 2, 3, 0)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training models for 2 remaining covariates: 100%|██████████| 2/2 [00:24<00:00, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Covariate 0 has 1 relevant permutations\n",
      "  Relevant permutations: [(1, 2, 0, 3)]\n",
      "  Covariate 3 has 1 relevant permutations\n",
      "  Relevant permutations: [(1, 2, 3, 0)]\n",
      "\n",
      "Sanity Check Results at depth 2:\n",
      "  Best found average log probability: -10.442317008972168\n",
      "  Average log probabilities for each starting covariate:\n",
      "    Covariate 0: -10.442317008972168\n",
      "    Covariate 3: -10.660240173339844\n",
      "  Best covariate at this stage: 0\n",
      "  Current ordering so far (including new covariate): [1, 2, 0]\n",
      "  True ordering so far (including new covariate): [1, 2, 0]\n",
      "The full inferred causal ordering is: [1, 2, 0, 3]\n",
      "The true causal ordering is: [1, 2, 0, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "discovered_ordering = determine_ordering(list(range(num_covariates)), true_ordering=true_ordering)\n",
    "print(f\"The full inferred causal ordering is: {discovered_ordering}\")\n",
    "print(f\"The true causal ordering is: {true_ordering}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
