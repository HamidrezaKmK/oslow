{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from notebooks.notebook_setup import device, smooth_graph, create_new_set_of_models, train_models_and_get_histories, update_dict\n",
    "from oslow.models.oslow import OSlowTest\n",
    "from oslow.data.synthetic.graph_generator import GraphGenerator\n",
    "from oslow.data.synthetic.utils import RandomGenerator\n",
    "from oslow.data.synthetic.parametric import AffineParametericDataset\n",
    "from oslow.data.synthetic.nonparametric import AffineNonParametericDataset\n",
    "from oslow.models.normalization import ActNorm\n",
    "from oslow.training.trainer import Trainer\n",
    "from oslow.config import GumbelTopKConfig, BirkhoffConfig, GumbelSinkhornStraightThroughConfig, ContrastiveDivergenceConfig\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "device = 'cuda:0'\n",
    "print(device)\n",
    "\n",
    "num_samples = 5000\n",
    "num_samples_topk = 128\n",
    "permutation_batch_size = 128\n",
    "flow_batch_size = 128\n",
    "epochs = 50\n",
    "flow_lr = 0.0001\n",
    "perm_lr = 0.000001\n",
    "flow_freq = 4\n",
    "perm_freq = 4\n",
    "num_nodes = 5\n",
    "\n",
    "graph_generator = GraphGenerator(\n",
    "    num_nodes=num_nodes,\n",
    "    seed=12,\n",
    "    graph_type=\"full\",\n",
    "    enforce_ordering=[i for i in range(num_nodes)],\n",
    ")\n",
    "graph = graph_generator.generate_dag()\n",
    "\n",
    "# These generators are also needed to generate the data\n",
    "gaussian_noise_generator = RandomGenerator('normal', seed=30, loc=0, scale=1)\n",
    "link_generator = RandomGenerator('uniform', seed=1100, low=0.5, high=1.5)\n",
    "\n",
    "# parameteric with sin(x) + x non-linearity and softplus\n",
    "dset_sinusoidal = AffineParametericDataset(\n",
    "    num_samples=num_samples,\n",
    "    graph=graph,\n",
    "    noise_generator=gaussian_noise_generator,\n",
    "    link_generator=link_generator,\n",
    "    link=\"sinusoid\",\n",
    "    perform_normalization=False,\n",
    "    standard=True,\n",
    ")\n",
    "class CustomTensorDataset(torch.utils.data.Dataset):\n",
    "    r\"\"\"Dataset wrapping tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "    Args:\n",
    "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tensor: torch.Tensor) -> None:\n",
    "        self.tensor = tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tensor[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "\n",
    "\n",
    "dataset = CustomTensorDataset(torch.tensor(dset_sinusoidal.samples.values).float())\n",
    "flow_dataloader = DataLoader(dataset, batch_size=flow_batch_size, shuffle=True)\n",
    "permutation_dataloader = DataLoader(dataset, batch_size=permutation_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 20:30:23 ERROR    Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhamidrezakamkari\u001b[0m (\u001b[33mordered-causal-discovery\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hamid/ocdaf/notebooks/wandb/run-20240208_203024-y8itehd5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ordered-causal-discovery/notebooks/runs/y8itehd5' target=\"_blank\">wild-grass-134</a></strong> to <a href='https://wandb.ai/ordered-causal-discovery/notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ordered-causal-discovery/notebooks' target=\"_blank\">https://wandb.ai/ordered-causal-discovery/notebooks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ordered-causal-discovery/notebooks/runs/y8itehd5' target=\"_blank\">https://wandb.ai/ordered-causal-discovery/notebooks/runs/y8itehd5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 63\u001b[0m\n\u001b[1;32m     37\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     38\u001b[0m                   dag\u001b[38;5;241m=\u001b[39mgraph,\n\u001b[1;32m     39\u001b[0m                   flow_dataloader\u001b[38;5;241m=\u001b[39mflow_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m                   device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     53\u001b[0m                   perform_final_buffer_search\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[1;32m     54\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebooks\u001b[39m\u001b[38;5;124m\"\u001b[39m, entity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mordered-causal-discovery\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m             tags\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     56\u001b[0m                 permutation_learning_config\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno-sigmoid\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m             ],)\n\u001b[0;32m---> 63\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/ocdaf/notebooks/../ocd/training/trainer.py:218\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch})\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_frequency):\n\u001b[0;32m--> 218\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_train_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_temperature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# logging.info(\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m#     f\"Flow step {epoch * self.flow_frequency + i} / {self.max_epochs * self.flow_frequency}, flow loss: {loss.item()}\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_scheduler, torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau\n\u001b[1;32m    225\u001b[0m     ):\n",
      "File \u001b[0;32m~/ocdaf/notebooks/../ocd/training/trainer.py:151\u001b[0m, in \u001b[0;36mTrainer.flow_train_step\u001b[0;34m(self, temperature, lbl)\u001b[0m\n\u001b[1;32m    149\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 151\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermutation_learning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow_learning_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/ocdaf/notebooks/../ocd/training/permutation.py:233\u001b[0m, in \u001b[0;36mPermutationMatrixLearningModule.flow_learning_loss\u001b[0;34m(self, model, batch, temperature)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow_learning_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: OSlow, batch: torch\u001b[38;5;241m.\u001b[39mTensor, temperature: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 233\u001b[0m     permutations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_permutations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_and_resample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgumbel_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    236\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlog_prob(batch, perm_mat\u001b[38;5;241m=\u001b[39mpermutations)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39mlog_probs\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/ocdaf/notebooks/../ocd/training/permutation.py:220\u001b[0m, in \u001b[0;36mPermutationMatrixLearningModule.sample_permutations\u001b[0;34m(self, num_samples, return_noises, unique_and_resample, gumbel_std)\u001b[0m\n\u001b[1;32m    211\u001b[0m     permutations \u001b[38;5;241m=\u001b[39m permutations[\n\u001b[1;32m    212\u001b[0m         torch\u001b[38;5;241m.\u001b[39mrandint(\n\u001b[1;32m    213\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         )\n\u001b[1;32m    218\u001b[0m     ]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# turn permutations into permutation matrices\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([turn_into_matrix(perm)\n\u001b[1;32m    221\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m perm \u001b[38;5;129;01min\u001b[39;00m permutations])\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# add some random noise to the permutation matrices\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_noises:\n",
      "File \u001b[0;32m~/ocdaf/notebooks/../ocd/training/permutation.py:220\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    211\u001b[0m     permutations \u001b[38;5;241m=\u001b[39m permutations[\n\u001b[1;32m    212\u001b[0m         torch\u001b[38;5;241m.\u001b[39mrandint(\n\u001b[1;32m    213\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         )\n\u001b[1;32m    218\u001b[0m     ]\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# turn permutations into permutation matrices\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mturn_into_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m                   \u001b[38;5;28;01mfor\u001b[39;00m perm \u001b[38;5;129;01min\u001b[39;00m permutations])\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# add some random noise to the permutation matrices\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_noises:\n",
      "File \u001b[0;32m~/ocdaf/notebooks/../ocd/training/utils.py:22\u001b[0m, in \u001b[0;36mturn_into_matrix\u001b[0;34m(permutation)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mturn_into_matrix\u001b[39m(permutation: torch\u001b[38;5;241m.\u001b[39mIntTensor):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meye\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermutation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpermutation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[permutation]\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;241m.\u001b[39mto(permutation\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     24\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%autoreload 2\n",
    "\n",
    "from oslow.models.oslow import OSlow\n",
    "import wandb\n",
    "\n",
    "torch.random.manual_seed(101)\n",
    "model = OSlow(in_features=num_nodes,\n",
    "              layers=[128, 64, 128],\n",
    "              dropout=None,\n",
    "              residual=False,\n",
    "              activation=torch.nn.LeakyReLU(),\n",
    "              additive=False,\n",
    "              num_transforms=1,\n",
    "              normalization=ActNorm,\n",
    "              base_distribution=torch.distributions.Normal(loc=0, scale=1),\n",
    "              ordering=None)\n",
    "\n",
    "\n",
    "def flow_optimizer(params): return torch.optim.AdamW(params, lr=flow_lr)\n",
    "def perm_optimizer(params): return torch.optim.AdamW(params, lr=perm_lr)\n",
    "\n",
    "\n",
    "permutation_learning_config = GumbelTopKConfig(\n",
    "    num_samples=num_samples_topk,\n",
    "    buffer_size=10,\n",
    "    buffer_update=10,\n",
    "    set_gamma_uniform=True,\n",
    ")\n",
    "\n",
    "\n",
    "# permutation_learning_config = GumbelSinkhornStraightThroughConfig(temp=0.1, iters=20)\n",
    "temperature_scheduler = 'linear'\n",
    "temperature = 1.0\n",
    "\n",
    "birkhoff_config = None if num_nodes > 4 else BirkhoffConfig(\n",
    "    num_samples=100, frequency=1, print_legend=False)\n",
    "trainer = Trainer(model=model,\n",
    "                  dag=graph,\n",
    "                  flow_dataloader=flow_dataloader,\n",
    "                  perm_dataloader=permutation_dataloader,\n",
    "                  flow_optimizer=flow_optimizer,\n",
    "                  permutation_optimizer=perm_optimizer,\n",
    "                  flow_frequency=flow_freq,\n",
    "                  temperature=temperature,\n",
    "                  temperature_scheduler=temperature_scheduler,\n",
    "                  permutation_frequency=perm_freq,\n",
    "                  max_epochs=epochs,\n",
    "                  flow_lr_scheduler=torch.optim.lr_scheduler.ConstantLR,\n",
    "                  permutation_lr_scheduler=torch.optim.lr_scheduler.ConstantLR,\n",
    "                  permutation_learning_config=permutation_learning_config,\n",
    "                  birkhoff_config=birkhoff_config,\n",
    "                  device=device,\n",
    "                  perform_final_buffer_search=True,)\n",
    "wandb.init(project=\"notebooks\", entity=\"ordered-causal-discovery\",\n",
    "            tags=[\n",
    "                permutation_learning_config.method,\n",
    "                f\"num_nodes-{num_nodes}\",\n",
    "                f\"epochs-{epochs}\",\n",
    "                f\"base-temperature-{temperature}\",\n",
    "                f\"temperature-scheduling-{temperature_scheduler}\",\n",
    "                \"no-sigmoid\",\n",
    "            ],)\n",
    "trainer.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oslow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
